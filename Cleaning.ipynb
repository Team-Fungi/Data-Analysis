{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Choosing/downloading dataset\n",
        "QIIME release from UNITE website, following text is copied from the unite website and is a good example of what we could write in the thesis:\n",
        "\n",
        "*Three sets of QIIME files are released, corresponding to the SHs resulting from clustering at the 3% distance (97% similarity) and 1% distance (99% similarity) threshold levels. The third set of files is the result of a dynamic use of clustering thresholds, such that some SHs are delimited at the 3% distance level, some at the 2.5% distance level, some at the 2% distance level, and so on; these choices were made manually by experts of those particular lineages of fungi. The syntax is the same throughout the three sets of files.*\n",
        "\n",
        "*Each SH is given a stable name of the accession number type, here shown in the FASTA file of the dynamic set:*\n",
        "\n",
        "\\>SH099456.05FU_FJ357315_refs \n",
        "CACAATATGAAGGCGGGCTGGCACTCCTTGAGAGGACCGGC…\n",
        "\n",
        "*SH099456 = accession number of the SH 05FU = global key release 5, organism group FUngi FJ357315 = GenBank/UNITE accession number of sequence chosen to represent the SH refs = this is a manually designated RefS (reps = this is an automatically chosen RepS*)\n",
        "\n",
        "*In the corresponding text file, the classification string of the SH is found:*\n",
        "\n",
        "*SH099456.05FU_FJ357315_refs k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Pleosporaceae;g__Embellisia;s__Embellisia_planifunda*\n",
        "\n",
        "*This specifies the hierarchical classification of the sequence. k = kingdom; p = phylum ; c = class ; o = order ; f = family ; g = genus ; and s = species. Missing information is indicated as \"unidentified\" item; “f__unidentified;” means that no family name for the sequence exists.*\n",
        "\n",
        "- something more about which version (most recent one) and why\n",
        "- something about the contents of the download (so 3 sets of QIIME release, all having 1 fasta file and 1 txt file, kind of like the explanation above)\n",
        "- maybe some more explanation about what each file (fasta and txt) file contains, kind of like the explanation above but then in a full text."
      ],
      "metadata": {
        "id": "CqbbRQoVCJFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Saving each QIIME release set into csv file\n",
        "- Because we find csv files are easier to handle than the txt + fasta seperately\n",
        "- CSV file are easy to manipulate and visualise with pandas\n",
        "- working with the 3 releases (99%, 97% and dynamic thresholds) because idk honestly, I'm just not sure which one to use yet so I'll convert all of them:) if we end op finding a good reason to choose one or the other, we can still delete them\n",
        "\n",
        "**CSV file contents**\n",
        "\n",
        "TaxonomyXX.csv is a csv file with all info from the fasta and txt file from QIIME release + length of sequences\n",
        "\n",
        "It has following columns: ['ACCNUM' 'KINGDOM' 'PHYLUM' 'CLASS' 'ORDER' 'FAMILY' 'GENUS' 'SPECIES', 'SEQ' 'LENGTH']\n",
        "\n",
        "Function was made where the following steps were taken (basically what the code below does):\n",
        "1. loading TXT file with pandas read_csv\n",
        "2. Removing prefixes from each column (k__, p__, ...)\n",
        "3. Entries with XX_XX_Incertae_sedis replaced with NaN for easier filtering with pandas as we will delete them anyway\n",
        "4. Create list of sequences from FASTA file and add them to the dataframe (note: txt has same order as fasta file so we don't need to worry about losing the order later as the order is contained when adding the list to the dataframe)\n",
        "5. Adding column 'SEQ' with sequences to dataframe\n",
        "6. Adding column 'LENGTH' with lengths of sequences: for easy visualisation later on, length can be determined quickly with the BioPython/BioSeq module (maybe some short intermezzo about BioPython can be added here?)\n",
        "7. saving pandas dataframe into csv with save_csv\n",
        "\n",
        "Same steps for each QIIME set!! So 3 times, code below only shows it for 1 file but I'll put it in some fancy function so we can just do it as 'QIIME_to_csv(file.txt, file.fasta)' so it's prettier:) So code below isn't the final version but the steps taken won't change\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mOo1WHkvsHKS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXIDlewyr4u9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "import matplotlib.pyplot as plt\n",
        "import squarify"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Taxonomy data into dataframe, make column names/header\n",
        "header = ['ACCNUM', 'KINGDOM', 'PHYLUM', 'CLASS', 'ORDER', 'FAMILY', 'GENUS', 'SPECIES']\n",
        "DataframeAll = pd.read_csv(\"sh_taxonomy_qiime_ver9_99_29.11.2022.txt\",\n",
        "                           delimiter=r'[\\t;]',\n",
        "                           engine='python',\n",
        "                           names=header)\n",
        "\n",
        "# Removing prefixes from each column (k__, p__, ...)\n",
        "for column in DataframeAll:\n",
        "    DataframeAll[column] = DataframeAll[column].str.lstrip(f'{str(column)[0].lower()}__')\n",
        "    if str(column) == 'SPECIES':\n",
        "        DataframeAll[column] = DataframeAll[column].str.rstrip('_sp')\n",
        "\n",
        "# Replacing XX_XX_Incertae_sedis with NaN\n",
        "for column in DataframeAll:\n",
        "    DataframeAll.loc[DataframeAll[str(column)].str.contains('sedis'), str(column)] = 'NaN'\n",
        "\n",
        "# Create list of sequences\n",
        "# Note: Fasta file has same order as txt file\n",
        "sequences = []\n",
        "for record in SeqIO.parse('sh_refs_qiime_ver9_99_29.11.2022.fasta', 'fasta'):\n",
        "    sequences.append(record.seq)\n",
        "\n",
        "# Adding column 'SEQ' with sequences to dataframe\n",
        "# Adding column 'LENGTH' with lengths of sequences\n",
        "# Note: format of sequences = BioSeq\n",
        "DataframeAll['SEQ'] = sequences\n",
        "DataframeAll['LENGTH'] = DataframeAll['SEQ'].str.len()\n",
        "\n",
        "# Saving csv file for further use\n",
        "DataframeAll.to_csv('Taxonomy99.csv', index=None)"
      ],
      "metadata": {
        "id": "LexGAST5skjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Filtering Pre-Analysis\n",
        "Filtering was done following these steps:\n",
        "1. check database for duplicates (these were printed to manually double check they were the same) and delete one of the duplicate duos so no more duplicates in the dataset\n",
        "2. Check for rows with NaN in dataset at any of the taxonomic levels and delete these rows, we need labelled data for training\n",
        "\n",
        "Save into csv for further use"
      ],
      "metadata": {
        "id": "ezaRSTClcXsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Taxonomy99.csv\")\n",
        "# Check database for duplicates\n",
        "print(df[df.duplicated(subset='SEQ', keep=False)])\n",
        "'''\n",
        "                              ACCNUM  ... LENGTH\n",
        "26779   SH2065224.09FU_KX065947_reps  ...    484\n",
        "59005   SH2065231.09FU_JN684766_refs  ...    484\n",
        "80104   SH1472029.09FU_MZ322935_reps  ...    471\n",
        "158833  SH1472048.09FU_JX272968_refs  ...    471\n",
        "\n",
        "[4 rows x 10 columns]\n",
        "'''\n",
        "# Delete one of the 2 of the duplicates\n",
        "df_noduplicates = df.drop_duplicates(subset='SEQ')\n",
        "print(df.shape)\n",
        "print(df_noduplicates.shape)\n",
        "'''\n",
        "(197557, 10)\n",
        "(197555, 10)\n",
        "'''\n",
        "# Check for rows with NaN in one of the columns\n",
        "print(df_noduplicates[df_noduplicates.isna().any(axis=1)])\n",
        "'''\n",
        "                                 ACCNUM  ... LENGTH\n",
        "4       SH1367668.09FU_UDB03376623_reps  ...    538\n",
        "5        SH1380440.09FU_UDB0684437_reps  ...    519\n",
        "6       SH2027613.09FU_UDB02118143_reps  ...    583\n",
        "9          SH1448937.09FU_MH178258_reps  ...    515\n",
        "10      SH1367682.09FU_UDB03384410_reps  ...    538\n",
        "...                                 ...  ...    ...\n",
        "197552   SH2003702.09FU_UDB0193281_reps  ...    609\n",
        "197553  SH2003697.09FU_UDB02276242_reps  ...    603\n",
        "197554  SH2003700.09FU_UDB02257150_reps  ...    611\n",
        "197555   SH2003704.09FU_UDB0283399_reps  ...    604\n",
        "197556  SH2003723.09FU_UDB02262136_reps  ...    605\n",
        "\n",
        "[93901 rows x 10 columns]\n",
        "'''\n",
        "# Drop all rows with NaN in one of the columns\n",
        "df_final = df_noduplicates.dropna()\n",
        "print(df_final)\n",
        "'''\n",
        "                                 ACCNUM  ... LENGTH\n",
        "0       SH2007016.09FU_UDB03582617_reps  ...    600\n",
        "1        SH1902468.09FU_UDB0227636_reps  ...    591\n",
        "2       SH1949090.09FU_UDB02678888_reps  ...    569\n",
        "3       SH1857229.09FU_UDB06071185_reps  ...    625\n",
        "7          SH2007337.09FU_AY340038_reps  ...    618\n",
        "...                                 ...  ...    ...\n",
        "197538  SH2003910.09FU_UDB03776145_reps  ...    559\n",
        "197539  SH2003916.09FU_UDB01950582_reps  ...    552\n",
        "197540     SH2003923.09FU_JQ666517_reps  ...    546\n",
        "197541  SH2003985.09FU_UDB03630955_reps  ...    541\n",
        "197546  SH2003333.09FU_UDB03237010_reps  ...    629\n",
        "\n",
        "[103654 rows x 10 columns]\n",
        "'''\n",
        "# Plot sizes of cleaning of dataframe (kinda stupid with the 2 duplicates?)\n",
        "df_sizes = pd.DataFrame({'Size':[103654,93901,2], 'Consisting of':[\"Final Dataset\", \"Entries with NaN\", \"Duplicates\"]})\n",
        "squarify.plot(sizes=df_sizes['Size'], label=df_sizes['Consisting of'], alpha=.8)\n",
        "plt.axis('off')\n",
        "plt.savefig('Sizes.png')\n",
        "'''\n",
        "Sizes.png saved in folder for idk cause it looks terrible\n",
        "'''\n",
        "# Save df_final to csv for further use and analysis\n",
        "df_final.to_csv('Taxonomy99Final.csv', index=None)"
      ],
      "metadata": {
        "id": "6qA2qyEscony"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DONE => analysis can start"
      ],
      "metadata": {
        "id": "nix1wu7vf68W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation after analysis (don't write this yet)\n",
        "Preparation of the data that hopefully follow out of the data analysis.\n",
        "\n",
        "*Just dropping them here because I'm going to forget them otherwise and I haven't made a seperate file yet for filtering after analysis. But these are some open questions that we should maybe ask prof or abiodun.*\n",
        "\n",
        "1. Should we maybe check if sequences containing a certain % of ambigous nucleotides are available? (a certain % of ambigous nucleotides could be due to sequencing errors and should be avoided) and delete these with higher %\n",
        "2. maybe delete ambigous nucleotides all together from sequences as it will simplify the kmer- and One-Hot-Encoding A LOT (ATGC instead of ATCGNFUCKTHIS). If we do, we should do it after/during the analysis and check the length distributions/mean/whatever and hopefully conclude it doesn't really change much so we can just delete them\n",
        "3. Cut the shorter and longer sequences because for OHE, we will need to padd them with additional N, as the neural network expects all same lengths. And we want same training/validation/test set for both OHE and kmers. Pretty distribution histogram (+literatures) will help us prove this point hopefully\n",
        "4. Maybe delete 'rare' examples (such as only 1 entry for a certain phylum), idk? Let's check how many there are for each taxonomic level and write some stuff on why these are not useful for training neural networks\n",
        "5. Maybe instead of deleting these 'rare' examples, we could augment the data (complement, reverse, reverse complement) to have more entries for the 'rare' examples"
      ],
      "metadata": {
        "id": "mx4d1r5af6Ci"
      }
    }
  ]
}
